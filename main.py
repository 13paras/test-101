from typing_extensions import TypedDict
from openai import AzureOpenAI
import openai
from dotenv import load_dotenv
import os
from typing import Literal
from langgraph.graph import StateGraph, START, END
import time
from pydantic import BaseModel
import neatlogs  # ‚úÖ imported and initialized only ‚Äî no usage below
import random

load_dotenv()

# ‚úÖ Initialize neatlogs (as required) ‚Äî but we won't use it anywhere else
neatlogs.init(api_key=os.getenv('NEATLOGS_API_KEY'))

# Azure OpenAI setup
client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
)

# Pydantic models
class DetectCallResponse(BaseModel):
    is_question_ai: bool
    
class CodingQuestionResponse(BaseModel):
    ai_message: str
    includes_code: bool = False
    topic_category: str = "general"

class AiResponse(BaseModel):
    ai_message: str

# State
class State(TypedDict):
    user_message: str
    ai_message: str
    is_coding_question: bool

# Fallback response generator
def generate_fallback_response(user_message: str, response_type: str) -> str:
    """Generate a fallback response when LLM fails to provide ai_message field"""
    
    # Check for common programming topics
    if "pydantic" in user_message.lower():
        return """Pydantic is a Python library that provides data validation and parsing using Python type hints.

Key Features:
- Data validation using type annotations
- Automatic data parsing and conversion
- JSON schema generation
- Integration with FastAPI and other frameworks

Basic Example:
```python
from pydantic import BaseModel
from typing import Optional

class User(BaseModel):
    id: int
    name: str
    email: Optional[str] = None

# Usage
user_data = {"id": "123", "name": "John Doe"}
user = User(**user_data)  # Automatically converts id to int
print(user.name)  # Output: John Doe
```

Common Use Cases:
- API request/response validation
- Configuration management
- Data pipeline validation
- FastAPI integration

Pydantic ensures data integrity and reduces runtime errors by validating data at the application boundary."""
    
    elif any(keyword in user_message.lower() for keyword in ["python", "programming", "code", "library", "framework"]):
        return f"I understand you're asking about a programming topic: '{user_message}'. However, I encountered an issue generating a complete response. Please try rephrasing your question or contact support if this issue persists."
    
    else:
        return f"I apologize, but I encountered an issue processing your request: '{user_message}'. Please try rephrasing your question or contact support if this problem continues."

# üéØ Global counter to trigger error after 2 successful runs
CALL_COUNTER = 0
MAX_SUCCESSFUL_CALLS = 2

# Node: Detect if query is coding-related
def detect_query(state: State):
    global CALL_COUNTER
    CALL_COUNTER += 1

    user_message = state.get("user_message")
    SYSTEM_PROMPT = """You are a query classifier. Determine if the user's query is coding/programming-related.
    
    Coding-related queries include:
    - Programming languages, libraries, frameworks
    - Software development concepts
    - Code examples, syntax, implementation questions
    - Technical tools and development practices
    
    Always provide a complete response with the classification result."""

    try:
        result = client.beta.chat.completions.parse(
            model="gpt-4o-mini",
            response_format=DetectCallResponse,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
        )
        state["is_coding_question"] = result.choices[0].message.parsed.is_question_ai
    except Exception as e:
        raise Exception(f"Error in detect_query: {str(e)}")

    return state

# Edge router
def route_edge(state: State) -> Literal["solve_coding_question", "solve_simple_question"]:
    is_coding_question = state.get("is_coding_question")
    if is_coding_question:
        return "solve_coding_question"
    else:
        return "solve_simple_question"

# Node: Solve coding question ‚Äî will force error after MAX_SUCCESSFUL_CALLS
def solve_coding_question(state: State):
    user_message = state.get("user_message")

    # üö® FORCE ERROR after MAX_SUCCESSFUL_CALLS
    if CALL_COUNTER > MAX_SUCCESSFUL_CALLS:
        raise Exception(f"üö® Simulated crash after {MAX_SUCCESSFUL_CALLS} successful runs!")

    SYSTEM_PROMPT = """You are a helpful programming assistant. Your task is to provide comprehensive, accurate answers to coding questions.
    
    Guidelines:
    - Always provide a complete response in the 'ai_message' field
    - Include explanations, code examples when relevant, and practical guidance
    - If you cannot provide a complete answer, explicitly state what you can help with
    - Be specific and actionable in your responses
    - Ensure your response is properly formatted and complete"""
    try:
        result = client.beta.chat.completions.parse(
            model="gpt-4o-mini",
            response_format=AiResponse,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
        )
        
        # Validate LLM response and check for ai_message field
        if not result or not hasattr(result, 'choices') or not result.choices:
            raise Exception("Failed to process model response: Empty or invalid response from LLM")
        
        parsed_response = result.choices[0].message.parsed
        if not parsed_response:
            raise Exception("Failed to process model response: LLM response could not be parsed")
            
        if not hasattr(parsed_response, 'ai_message') or not parsed_response.ai_message:
            print(f"‚ö†Ô∏è Warning: LLM response missing required field 'ai_message'. Model may have returned partial output.")
            # Provide a fallback response based on the user query
            fallback_response = generate_fallback_response(user_message, "coding")
            state["ai_message"] = fallback_response
        else:
            state["ai_message"] = parsed_response.ai_message
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error in solve_coding_question: {str(e)}")
        # Provide a fallback response to ensure graceful handling
        fallback_response = generate_fallback_response(user_message, "coding")
        state["ai_message"] = fallback_response

    return state

# Node: Solve simple question
def solve_simple_question(state: State):
    user_message = state.get("user_message")
    SYSTEM_PROMPT = """You are a helpful assistant. Your task is to provide clear, accurate answers to user questions.
    
    Guidelines:
    - Always provide a complete response in the 'ai_message' field
    - Be informative and helpful
    - If you cannot provide a complete answer, explain what you can help with
    - Ensure your response is properly formatted and complete
    - Maintain a friendly and professional tone"""

    try:
        result = client.beta.chat.completions.parse(
            model="gpt-4o-mini",
            response_format=AiResponse,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
        )
        
        # Validate LLM response and check for ai_message field
        if not result or not hasattr(result, 'choices') or not result.choices:
            raise Exception("Failed to process model response: Empty or invalid response from LLM")
        
        parsed_response = result.choices[0].message.parsed
        if not parsed_response:
            raise Exception("Failed to process model response: LLM response could not be parsed")
            
        if not hasattr(parsed_response, 'ai_message') or not parsed_response.ai_message:
            print(f"‚ö†Ô∏è Warning: LLM response missing required field 'ai_message'. Model may have returned partial output.")
            # Provide a fallback response based on the user query
            fallback_response = generate_fallback_response(user_message, "simple")
            state["ai_message"] = fallback_response
        else:
            state["ai_message"] = parsed_response.ai_message
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error in solve_simple_question: {str(e)}")
        # Provide a fallback response to ensure graceful handling
        fallback_response = generate_fallback_response(user_message, "simple")
        state["ai_message"] = fallback_response

    return state

# Build graph
graph_builder = StateGraph(State)
graph_builder.add_node("detect_query", detect_query)
graph_builder.add_node("solve_coding_question", solve_coding_question)
graph_builder.add_node("solve_simple_question", solve_simple_question)

graph_builder.add_edge(START, "detect_query")
graph_builder.add_conditional_edges("detect_query", route_edge)
graph_builder.add_edge("solve_coding_question", END)
graph_builder.add_edge("solve_simple_question", END)

graph = graph_builder.compile()

# Function to run graph with error handling
def call_graph():
    state = {
        "user_message": "Explain me about Pydantic in Python",
        "ai_message": "",
        "is_coding_question": False
    }

    try:
        print("üöÄ Running graph...")
        result = graph.invoke(state)
        print("‚úÖ Success:", result["ai_message"][:100] + "...")
    except Exception as e:
        print("‚ùå ERROR CAUGHT:", str(e))  # <-- Pure Python print, no neatlogs

# Run multiple times
if __name__ == "__main__":
    for i in range(5):
        print(f"\n{'='*40}")
        print(f"        RUN #{i+1}")
        print('='*40)
        call_graph()
        time.sleep(2)